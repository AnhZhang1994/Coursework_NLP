{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502d503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정 후\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "#kobert\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "#GPU 사용\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4939ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "n_devices = torch.cuda.device_count()\n",
    "print(n_devices)\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a745ffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/seclab_taewan/Taewan/NLP/.cache/kobert_v1.zip\n",
      "using cached model. /home/seclab_taewan/Taewan/NLP/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2057e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tag_data = pd.read_csv('./tag_bio_jaso.csv')\n",
    "tag_data.columns = ['Sentence #', 'word', 'bio_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57a4f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'PER_B', 'PER_I', 'ADDR_B', 'ADDR_I', 'BIR_B', 'BIR_I',\n",
       "       'MON_B', 'MON_I'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# data[data.tag==math.nan]\n",
    "tag_data.bio_tag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4380d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data.loc[(tag_data['bio_tag'] == \"PER\"), 'bio_tag'] = 0\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"PER_B\"), 'bio_tag'] = 0\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"PER_I\"), 'bio_tag'] = 0\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"ADDR\"), 'bio_tag'] = 1\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"ADDR_B\"), 'bio_tag'] = 1\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"ADDR_I\"), 'bio_tag'] = 1\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"MON\"), 'bio_tag'] = 2\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"MON_B\"), 'bio_tag'] = 2\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"MON_I\"), 'bio_tag'] = 2\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"BIR\"), 'bio_tag'] = 3\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"BIR_B\"), 'bio_tag'] = 3\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"BIR_I\"), 'bio_tag'] = 3\n",
    "tag_data.loc[(tag_data['bio_tag'] == \"O\"), 'bio_tag'] = 4\n",
    "\n",
    "\n",
    "data_list = []\n",
    "for q, label in zip(tag_data['word'], tag_data['bio_tag']):\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)\n",
    "\n",
    "# for d in data_list:\n",
    "#     if d[1] in 'nan':\n",
    "#         print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3469e1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 3, 2], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_data.bio_tag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff6c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train, test 데이터로 나누기 Word\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import random\n",
    "\n",
    "# # random.Random(len(data_list)).shuffle(data_list)\n",
    "                                                         \n",
    "# dataset_train, dataset_test = train_test_split(data_list, test_size=0.200006, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90883a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train, test 데이터로 나누기 char\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import random\n",
    "\n",
    "# # random.Random(len(data_list)).shuffle(data_list)\n",
    "                                                         \n",
    "# dataset_train, dataset_test = train_test_split(data_list, test_size=0.2000085, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18b8b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 데이터로 나누기 jaso\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# random.Random(len(data_list)).shuffle(data_list)\n",
    "                                                         \n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.200005, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45d3a288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7547264\n",
      "1886848\n"
     ]
    }
   ],
   "source": [
    "dataset_test = dataset_test[:-27]\n",
    "\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffae64a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per 2719342\n",
      "addr 71299\n",
      "mon 1366984\n",
      "bir 1554418\n",
      "o 1835221\n"
     ]
    }
   ],
   "source": [
    "per = 0\n",
    "addr = 0\n",
    "mon = 0\n",
    "bir = 0\n",
    "o = 0\n",
    "\n",
    "for d in dataset_train:\n",
    "    if str(d[1]) == str(0):\n",
    "        per += 1\n",
    "    if str(d[1]) == str(1):\n",
    "        addr += 1\n",
    "    if str(d[1]) == str(2):\n",
    "        mon += 1\n",
    "    if str(d[1]) == str(3):\n",
    "        bir += 1\n",
    "    if str(d[1]) == str(4):\n",
    "        o += 1\n",
    "        \n",
    "print('per', per)\n",
    "print('addr', addr)\n",
    "print('mon', mon)\n",
    "print('bir', bir)\n",
    "print('o', o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "501fdb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per 680821\n",
      "addr 17686\n",
      "mon 342131\n",
      "bir 387933\n",
      "o 458277\n"
     ]
    }
   ],
   "source": [
    "per = 0\n",
    "addr = 0\n",
    "mon = 0\n",
    "bir = 0\n",
    "o = 0\n",
    "\n",
    "for d in dataset_test:\n",
    "    if str(d[1]) == str(0):\n",
    "        per += 1\n",
    "    if str(d[1]) == str(1):\n",
    "        addr += 1\n",
    "    if str(d[1]) == str(2):\n",
    "        mon += 1\n",
    "    if str(d[1]) == str(3):\n",
    "        bir += 1\n",
    "    if str(d[1]) == str(4):\n",
    "        o += 1\n",
    "        \n",
    "print('per', per)\n",
    "print('addr', addr)\n",
    "print('mon', mon)\n",
    "print('bir', bir)\n",
    "print('o', o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "886682c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.word = [transform([str(i[sent_idx])]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.word[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8f3df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 380\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c73d789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/seclab_taewan/Taewan/NLP/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9198c6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2, 553,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
       "       dtype=int32),\n",
       " array(3, dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=int32),\n",
       " 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec94e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a397385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=5,   ##클래스 수##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cf2e892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f4419a716d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "#BERT 모델 불러오기\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "# model = BERTClassifier(bertmodel,  dr_rate=0.5)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate, warmup=-1, t_total=-1,\n",
    "                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01,\n",
    "                 max_grad_norm=1.0)\n",
    "\n",
    "# class_weights = [9518, 4134, 13873, 12537, 21314]\n",
    "# normedWeights = [1 - (x / sum(class_weights)) for x in class_weights]\n",
    "# normedWeights = torch.FloatTensor(normedWeights).to(device)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=normedWeights)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "#정확도 측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "    \n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123676/1621645026.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6fb24215ce4499a81b752201d699e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117926 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/etc/anaconda3/envs/test/lib/python3.9/site-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.6085920333862305 train acc 0.234375\n",
      "epoch 1 batch id 381 loss 1.3370451927185059 train acc 0.33968996062992124\n",
      "epoch 1 batch id 761 loss 1.0826455354690552 train acc 0.43035479632063073\n",
      "epoch 1 batch id 1141 loss 0.8616599440574646 train acc 0.4768021472392638\n",
      "epoch 1 batch id 1521 loss 0.8916258811950684 train acc 0.5037906804733728\n",
      "epoch 1 batch id 1901 loss 0.756023645401001 train acc 0.52014564702788\n",
      "epoch 1 batch id 2281 loss 0.907560408115387 train acc 0.5305923936869794\n",
      "epoch 1 batch id 2661 loss 0.8068732619285583 train acc 0.5383197106350995\n",
      "epoch 1 batch id 3041 loss 0.8482539653778076 train acc 0.5442442864189412\n",
      "epoch 1 batch id 3421 loss 0.71912682056427 train acc 0.5491586889798304\n",
      "epoch 1 batch id 3801 loss 0.8306897878646851 train acc 0.5529137069192318\n",
      "epoch 1 batch id 4181 loss 0.9292442798614502 train acc 0.5565766264051663\n",
      "epoch 1 batch id 4561 loss 0.8511300683021545 train acc 0.5595195680771761\n",
      "epoch 1 batch id 4941 loss 0.8029175400733948 train acc 0.5618106152600688\n",
      "epoch 1 batch id 5321 loss 0.7278369069099426 train acc 0.5635864968990791\n",
      "epoch 1 batch id 5701 loss 0.7484583854675293 train acc 0.5653421548851079\n",
      "epoch 1 batch id 6081 loss 0.9314634799957275 train acc 0.566821863180398\n",
      "epoch 1 batch id 6461 loss 0.7152189016342163 train acc 0.5685870027859464\n",
      "epoch 1 batch id 6841 loss 0.8355748653411865 train acc 0.5700486953661745\n",
      "epoch 1 batch id 7221 loss 0.8053224086761475 train acc 0.5710925598947514\n",
      "epoch 1 batch id 7601 loss 0.8528271913528442 train acc 0.5722746184712538\n",
      "epoch 1 batch id 7981 loss 0.8062306046485901 train acc 0.5731972810424759\n",
      "epoch 1 batch id 8361 loss 0.8044539093971252 train acc 0.574256593110872\n",
      "epoch 1 batch id 8741 loss 0.6796367168426514 train acc 0.5749538811348816\n",
      "epoch 1 batch id 9121 loss 0.7551124691963196 train acc 0.5757112706940029\n",
      "epoch 1 batch id 9501 loss 0.7186129689216614 train acc 0.5762830886222503\n",
      "epoch 1 batch id 9881 loss 0.7682961821556091 train acc 0.5771366764497521\n",
      "epoch 1 batch id 10261 loss 0.748904287815094 train acc 0.5777184241302017\n",
      "epoch 1 batch id 10641 loss 0.8740750551223755 train acc 0.5784098651442534\n",
      "epoch 1 batch id 11021 loss 0.8655735850334167 train acc 0.5790876508483803\n",
      "epoch 1 batch id 11401 loss 0.9480394721031189 train acc 0.5797339597403737\n",
      "epoch 1 batch id 11781 loss 0.7328398823738098 train acc 0.5801263687293099\n",
      "epoch 1 batch id 12161 loss 0.9072225093841553 train acc 0.5804621330482691\n",
      "epoch 1 batch id 12541 loss 0.8497624397277832 train acc 0.5808273861733514\n",
      "epoch 1 batch id 12921 loss 0.7928289771080017 train acc 0.5813271515362588\n",
      "epoch 1 batch id 13301 loss 0.7626796364784241 train acc 0.5815822118637697\n",
      "epoch 1 batch id 13681 loss 0.8485308885574341 train acc 0.5818596502448652\n",
      "epoch 1 batch id 14061 loss 0.7321275472640991 train acc 0.582326559277434\n",
      "epoch 1 batch id 14441 loss 0.7221840023994446 train acc 0.5826390571982549\n",
      "epoch 1 batch id 14821 loss 0.69027179479599 train acc 0.582896523513933\n",
      "epoch 1 batch id 15201 loss 0.7647246718406677 train acc 0.5831339221103875\n",
      "epoch 1 batch id 15581 loss 0.846390962600708 train acc 0.583385814453501\n",
      "epoch 1 batch id 15961 loss 0.8245056867599487 train acc 0.583698154877514\n",
      "epoch 1 batch id 16341 loss 0.7721932530403137 train acc 0.5839538966403525\n",
      "epoch 1 batch id 16721 loss 0.8049116134643555 train acc 0.5841821287602416\n",
      "epoch 1 batch id 17101 loss 0.9737825989723206 train acc 0.5843654976317174\n",
      "epoch 1 batch id 17481 loss 0.69189453125 train acc 0.5845918425719352\n",
      "epoch 1 batch id 17861 loss 0.7447479963302612 train acc 0.5847648157997872\n",
      "epoch 1 batch id 18241 loss 0.7697354555130005 train acc 0.5850522175319336\n",
      "epoch 1 batch id 18621 loss 0.812680721282959 train acc 0.5851751718489877\n",
      "epoch 1 batch id 19001 loss 0.8884455561637878 train acc 0.5852117980632598\n",
      "epoch 1 batch id 19381 loss 0.8471548557281494 train acc 0.5853534066869615\n",
      "epoch 1 batch id 19761 loss 0.7925757169723511 train acc 0.5854990574869693\n",
      "epoch 1 batch id 20141 loss 0.8228638768196106 train acc 0.5857284270890224\n",
      "epoch 1 batch id 20521 loss 0.7900011539459229 train acc 0.5860003167486965\n",
      "epoch 1 batch id 20901 loss 0.9234574437141418 train acc 0.5861030871728625\n",
      "epoch 1 batch id 21281 loss 0.7154541611671448 train acc 0.5862264167567314\n",
      "epoch 1 batch id 21661 loss 0.8534170985221863 train acc 0.5864290949171321\n",
      "epoch 1 batch id 22041 loss 0.7396121025085449 train acc 0.5865460959121637\n",
      "epoch 1 batch id 22421 loss 0.7645308971405029 train acc 0.5866500713616699\n",
      "epoch 1 batch id 22801 loss 0.7169530391693115 train acc 0.5868026621639402\n",
      "epoch 1 batch id 23181 loss 0.8370901346206665 train acc 0.5869205922954144\n",
      "epoch 1 batch id 23561 loss 0.9318963289260864 train acc 0.587011507363864\n",
      "epoch 1 batch id 23941 loss 0.886551022529602 train acc 0.5870336191053005\n",
      "epoch 1 batch id 24321 loss 0.7131833434104919 train acc 0.587183529665721\n",
      "epoch 1 batch id 24701 loss 0.7772941589355469 train acc 0.5872301475648759\n",
      "epoch 1 batch id 25081 loss 0.8948190808296204 train acc 0.5873208305091504\n",
      "epoch 1 batch id 25461 loss 0.8590182065963745 train acc 0.587427217116374\n",
      "epoch 1 batch id 25841 loss 0.918562114238739 train acc 0.5875613124492086\n",
      "epoch 1 batch id 26221 loss 0.774962842464447 train acc 0.5876790072842378\n",
      "epoch 1 batch id 26601 loss 0.7851744294166565 train acc 0.5877592712679974\n",
      "epoch 1 batch id 26981 loss 0.7707412242889404 train acc 0.5878621761610022\n",
      "epoch 1 batch id 27361 loss 0.6531379818916321 train acc 0.5879742151237163\n",
      "epoch 1 batch id 27741 loss 0.7286320328712463 train acc 0.5880888170938322\n",
      "epoch 1 batch id 28121 loss 0.8298780918121338 train acc 0.5882225472067139\n",
      "epoch 1 batch id 28501 loss 0.793291449546814 train acc 0.5882951475386828\n",
      "epoch 1 batch id 28881 loss 0.7959168553352356 train acc 0.5883836908348049\n",
      "epoch 1 batch id 29261 loss 0.8157359957695007 train acc 0.5884907598851714\n",
      "epoch 1 batch id 29641 loss 0.7960265874862671 train acc 0.588563982321784\n",
      "epoch 1 batch id 30021 loss 0.881359338760376 train acc 0.5886577312547883\n",
      "epoch 1 batch id 30401 loss 0.8264000415802002 train acc 0.5887573599552647\n",
      "epoch 1 batch id 30781 loss 0.7570840716362 train acc 0.5888453916376986\n",
      "epoch 1 batch id 31161 loss 0.7722011804580688 train acc 0.5887938849844356\n",
      "epoch 1 batch id 31541 loss 0.6681297421455383 train acc 0.5887669026029613\n",
      "epoch 1 batch id 31921 loss 0.7759794592857361 train acc 0.5888614665893925\n",
      "epoch 1 batch id 32301 loss 0.811964213848114 train acc 0.5889175257731959\n",
      "epoch 1 batch id 32681 loss 0.7381458282470703 train acc 0.5889608067378599\n",
      "epoch 1 batch id 33061 loss 0.7997241616249084 train acc 0.5890631143946039\n",
      "epoch 1 batch id 33441 loss 0.7687268257141113 train acc 0.5891252504410753\n",
      "epoch 1 batch id 33821 loss 0.7910007834434509 train acc 0.589229417373821\n",
      "epoch 1 batch id 34201 loss 0.8745037317276001 train acc 0.5893436047191603\n",
      "epoch 1 batch id 34581 loss 0.8604945540428162 train acc 0.5894918813799486\n",
      "epoch 1 batch id 34961 loss 0.921317994594574 train acc 0.5895238623037099\n",
      "epoch 1 batch id 35341 loss 0.7010177373886108 train acc 0.5895759351744433\n",
      "epoch 1 batch id 35721 loss 0.7462226152420044 train acc 0.5896338988270206\n",
      "epoch 1 batch id 36101 loss 1.0271337032318115 train acc 0.5897382517104789\n",
      "epoch 1 batch id 36481 loss 0.8514071106910706 train acc 0.5897466318083386\n",
      "epoch 1 batch id 36861 loss 0.7634074687957764 train acc 0.5898184225875587\n",
      "epoch 1 batch id 37241 loss 0.8564664721488953 train acc 0.5899168591068983\n",
      "epoch 1 batch id 37621 loss 0.8524284958839417 train acc 0.5900141377156375\n",
      "epoch 1 batch id 38001 loss 0.7750009298324585 train acc 0.5901016585089867\n",
      "epoch 1 batch id 38381 loss 0.8373510837554932 train acc 0.5901622059091738\n",
      "epoch 1 batch id 38761 loss 0.6951720714569092 train acc 0.590201007455948\n",
      "epoch 1 batch id 39141 loss 0.7176712155342102 train acc 0.5902370596050177\n",
      "epoch 1 batch id 39521 loss 0.9479086995124817 train acc 0.5901356240985804\n",
      "epoch 1 batch id 39901 loss 0.7083272933959961 train acc 0.5901598644144257\n",
      "epoch 1 batch id 40281 loss 0.6648915410041809 train acc 0.5901506758769643\n",
      "epoch 1 batch id 40661 loss 0.8073225617408752 train acc 0.5901516502299501\n",
      "epoch 1 batch id 41041 loss 0.7814024090766907 train acc 0.5901743073999172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 41421 loss 0.9719656109809875 train acc 0.5901822143357234\n",
      "epoch 1 batch id 41801 loss 0.8264890909194946 train acc 0.590190351307385\n",
      "epoch 1 batch id 42181 loss 0.695038914680481 train acc 0.5902361252696712\n",
      "epoch 1 batch id 42561 loss 0.895255446434021 train acc 0.5902601560113719\n",
      "epoch 1 batch id 42941 loss 0.7835972309112549 train acc 0.590219356209683\n",
      "epoch 1 batch id 43321 loss 0.9213414192199707 train acc 0.5901900925648069\n",
      "epoch 1 batch id 43701 loss 1.081103801727295 train acc 0.590078030251024\n",
      "epoch 1 batch id 44081 loss 0.992877185344696 train acc 0.5894039523831129\n",
      "epoch 1 batch id 44461 loss 1.0277416706085205 train acc 0.5887603742605879\n",
      "epoch 1 batch id 44841 loss 1.2978284358978271 train acc 0.5875001393813697\n",
      "epoch 1 batch id 45221 loss 1.253525733947754 train acc 0.5863170595519781\n",
      "epoch 1 batch id 45601 loss 1.0783226490020752 train acc 0.5856796588890595\n",
      "epoch 1 batch id 45981 loss 1.0833799839019775 train acc 0.5850419194884844\n",
      "epoch 1 batch id 46361 loss 0.8383045792579651 train acc 0.5844918142404176\n",
      "epoch 1 batch id 46741 loss 1.1783416271209717 train acc 0.5843491260349586\n",
      "epoch 1 batch id 47121 loss 1.0653753280639648 train acc 0.5838837779334055\n",
      "epoch 1 batch id 47501 loss 0.9406031370162964 train acc 0.5832633786657123\n",
      "epoch 1 batch id 47881 loss 1.1577216386795044 train acc 0.5826290047200351\n",
      "epoch 1 batch id 48261 loss 0.905043363571167 train acc 0.5819450487971655\n",
      "epoch 1 batch id 48641 loss 0.9353389739990234 train acc 0.5813170730453733\n",
      "epoch 1 batch id 49021 loss 1.0524095296859741 train acc 0.5808052926296893\n",
      "epoch 1 batch id 49401 loss 0.9918469190597534 train acc 0.5802403417947005\n",
      "epoch 1 batch id 49781 loss 0.9120931029319763 train acc 0.5797364330768767\n",
      "epoch 1 batch id 50161 loss 1.078301191329956 train acc 0.5792541765515041\n",
      "epoch 1 batch id 50541 loss 0.9340682029724121 train acc 0.5787136310124453\n",
      "epoch 1 batch id 50921 loss 1.0906414985656738 train acc 0.5781753230494295\n",
      "epoch 1 batch id 51301 loss 0.9599536657333374 train acc 0.5776641780861972\n",
      "epoch 1 batch id 51681 loss 1.0817489624023438 train acc 0.5771668988603161\n",
      "epoch 1 batch id 52061 loss 1.0777406692504883 train acc 0.5766987884404833\n",
      "epoch 1 batch id 52441 loss 1.0318959951400757 train acc 0.5762571270570737\n",
      "epoch 1 batch id 52821 loss 0.9236023426055908 train acc 0.5757786320781507\n",
      "epoch 1 batch id 53201 loss 1.1936266422271729 train acc 0.5753137276554952\n",
      "epoch 1 batch id 53581 loss 0.8950755596160889 train acc 0.57483442125007\n",
      "epoch 1 batch id 53961 loss 1.0287048816680908 train acc 0.5743879260021126\n",
      "epoch 1 batch id 54341 loss 1.0597020387649536 train acc 0.5739560138753428\n",
      "epoch 1 batch id 54721 loss 1.0611307621002197 train acc 0.5735557989620073\n",
      "epoch 1 batch id 55101 loss 1.0599493980407715 train acc 0.5731060915409883\n",
      "epoch 1 batch id 55481 loss 0.9495428204536438 train acc 0.5727008457850435\n",
      "epoch 1 batch id 55861 loss 1.1250172853469849 train acc 0.5722806944916847\n",
      "epoch 1 batch id 56241 loss 1.0031859874725342 train acc 0.5718506627727103\n",
      "epoch 1 batch id 56621 loss 1.0780038833618164 train acc 0.5714597940693382\n",
      "epoch 1 batch id 57001 loss 0.9513350129127502 train acc 0.5711020968930369\n",
      "epoch 1 batch id 57381 loss 1.4104877710342407 train acc 0.5705389196772451\n",
      "epoch 1 batch id 57761 loss 1.503824234008789 train acc 0.5691602573535777\n",
      "epoch 1 batch id 58141 loss 1.3830832242965698 train acc 0.5677861792882819\n",
      "epoch 1 batch id 58521 loss 1.4433073997497559 train acc 0.5664278101023564\n",
      "epoch 1 batch id 58901 loss 1.4264380931854248 train acc 0.5650763569379128\n",
      "epoch 1 batch id 59281 loss 1.3738890886306763 train acc 0.5637952084141631\n",
      "epoch 1 batch id 59661 loss 1.381583333015442 train acc 0.562479833978646\n",
      "epoch 1 batch id 60041 loss 1.3800711631774902 train acc 0.5611909986509219\n",
      "epoch 1 batch id 60421 loss 1.3720226287841797 train acc 0.5599659576140745\n",
      "epoch 1 batch id 60801 loss 1.497120976448059 train acc 0.5587287318465157\n",
      "epoch 1 batch id 61181 loss 1.3676836490631104 train acc 0.5575010011278011\n",
      "epoch 1 batch id 61561 loss 1.3999587297439575 train acc 0.5563044175695652\n",
      "epoch 1 batch id 61941 loss 1.453170657157898 train acc 0.5550715600329346\n",
      "epoch 1 batch id 62321 loss 1.415462851524353 train acc 0.5538569964377978\n",
      "epoch 1 batch id 62701 loss 1.3372265100479126 train acc 0.5526546626050621\n",
      "epoch 1 batch id 63081 loss 1.332551121711731 train acc 0.5514987674577131\n",
      "epoch 1 batch id 63461 loss 1.30059015750885 train acc 0.5503675485731394\n",
      "epoch 1 batch id 63841 loss 1.359054684638977 train acc 0.5492706000062656\n",
      "epoch 1 batch id 64221 loss 1.3638873100280762 train acc 0.5481567069182978\n",
      "epoch 1 batch id 64601 loss 1.3995157480239868 train acc 0.5470476946951286\n",
      "epoch 1 batch id 64981 loss 1.4015147686004639 train acc 0.5459348213323895\n",
      "epoch 1 batch id 65361 loss 1.5002144575119019 train acc 0.5448798308624409\n",
      "epoch 1 batch id 65741 loss 1.3821096420288086 train acc 0.5437866495033541\n",
      "epoch 1 batch id 66121 loss 1.4729427099227905 train acc 0.5427483326023502\n",
      "epoch 1 batch id 66501 loss 1.394126534461975 train acc 0.5417237616727568\n",
      "epoch 1 batch id 66881 loss 1.4071215391159058 train acc 0.540711067044452\n",
      "epoch 1 batch id 67261 loss 1.3357264995574951 train acc 0.5396991291387282\n",
      "epoch 1 batch id 67641 loss 1.4170970916748047 train acc 0.5386962511642347\n",
      "epoch 1 batch id 68021 loss 1.4678372144699097 train acc 0.5376969979859161\n",
      "epoch 1 batch id 68401 loss 1.3759019374847412 train acc 0.5367019944883846\n",
      "epoch 1 batch id 68781 loss 1.3347612619400024 train acc 0.5357218472397901\n",
      "epoch 1 batch id 69161 loss 1.3707090616226196 train acc 0.5347513410737266\n",
      "epoch 1 batch id 69541 loss 1.4219412803649902 train acc 0.5338029004472182\n",
      "epoch 1 batch id 69921 loss 1.448835015296936 train acc 0.5328665565423836\n",
      "epoch 1 batch id 70301 loss 1.4182195663452148 train acc 0.5319354454417433\n",
      "epoch 1 batch id 70681 loss 1.3558759689331055 train acc 0.5310293784751207\n",
      "epoch 1 batch id 71061 loss 1.4188090562820435 train acc 0.5300822092990529\n",
      "epoch 1 batch id 71441 loss 1.404788613319397 train acc 0.529176392057782\n",
      "epoch 1 batch id 71821 loss 1.421510934829712 train acc 0.5282962591024909\n",
      "epoch 1 batch id 72201 loss 1.3553849458694458 train acc 0.5274080777967064\n",
      "epoch 1 batch id 72581 loss 1.380394697189331 train acc 0.5265468493820696\n",
      "epoch 1 batch id 72961 loss 1.4091401100158691 train acc 0.5256708207124354\n",
      "epoch 1 batch id 73341 loss 1.4159274101257324 train acc 0.5248023786149629\n",
      "epoch 1 batch id 73721 loss 1.3760738372802734 train acc 0.5239558182200459\n",
      "epoch 1 batch id 74101 loss 1.4653818607330322 train acc 0.523135441829395\n",
      "epoch 1 batch id 74481 loss 1.3068568706512451 train acc 0.5223202897383226\n",
      "epoch 1 batch id 74861 loss 1.3617677688598633 train acc 0.5214923324561521\n",
      "epoch 1 batch id 75241 loss 1.3646384477615356 train acc 0.5206667159527385\n",
      "epoch 1 batch id 75621 loss 1.3644444942474365 train acc 0.5198888618902157\n",
      "epoch 1 batch id 76001 loss 1.3585344552993774 train acc 0.5190661553795345\n",
      "epoch 1 batch id 76381 loss 1.3884339332580566 train acc 0.5182712732878595\n",
      "epoch 1 batch id 76761 loss 1.4844895601272583 train acc 0.5175194760360079\n",
      "epoch 1 batch id 77141 loss 1.3342952728271484 train acc 0.5167489564563591\n",
      "epoch 1 batch id 77521 loss 1.4436122179031372 train acc 0.5159890142671019\n",
      "epoch 1 batch id 77901 loss 1.376150369644165 train acc 0.5152394946791441\n",
      "epoch 1 batch id 78281 loss 1.3582744598388672 train acc 0.5144980502931746\n",
      "epoch 1 batch id 78661 loss 1.420548677444458 train acc 0.5137748932126467\n",
      "epoch 1 batch id 79041 loss 1.368040919303894 train acc 0.5130480146379727\n",
      "epoch 1 batch id 79421 loss 1.356959581375122 train acc 0.5123155006232608\n",
      "epoch 1 batch id 79801 loss 1.3913490772247314 train acc 0.5115970116289269\n",
      "epoch 1 batch id 80181 loss 1.3406673669815063 train acc 0.5108596098202817\n",
      "epoch 1 batch id 80561 loss 1.407735824584961 train acc 0.5101245096883107\n",
      "epoch 1 batch id 80941 loss 1.4292070865631104 train acc 0.5094447653228895\n",
      "epoch 1 batch id 81321 loss 1.3426553010940552 train acc 0.5087446661993827\n",
      "epoch 1 batch id 81701 loss 1.3377940654754639 train acc 0.5080568169300254\n",
      "epoch 1 batch id 82081 loss 1.5105191469192505 train acc 0.5073709582607424\n",
      "epoch 1 batch id 82461 loss 1.3967108726501465 train acc 0.5066936945950207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 82841 loss 1.4021590948104858 train acc 0.506012647722746\n",
      "epoch 1 batch id 83221 loss 1.4054502248764038 train acc 0.5053432652215186\n",
      "epoch 1 batch id 83601 loss 1.3736661672592163 train acc 0.5046979103120776\n",
      "epoch 1 batch id 83981 loss 1.3368300199508667 train acc 0.5040438834379205\n",
      "epoch 1 batch id 84361 loss 1.396559715270996 train acc 0.5033894512867321\n",
      "epoch 1 batch id 84741 loss 1.375125765800476 train acc 0.5027416259543787\n",
      "epoch 1 batch id 85121 loss 1.401983618736267 train acc 0.5020916915332292\n",
      "epoch 1 batch id 85501 loss 1.4454257488250732 train acc 0.5014391279049368\n",
      "epoch 1 batch id 85881 loss 1.4829171895980835 train acc 0.5008283627927015\n",
      "epoch 1 batch id 86261 loss 1.3529630899429321 train acc 0.5002008801776006\n",
      "epoch 1 batch id 86641 loss 1.4108151197433472 train acc 0.4995888205353124\n",
      "epoch 1 batch id 87021 loss 1.3548905849456787 train acc 0.49899126360303836\n",
      "epoch 1 batch id 87401 loss 1.382770299911499 train acc 0.4983937183213007\n",
      "epoch 1 batch id 87781 loss 1.3630586862564087 train acc 0.4978020585320286\n",
      "epoch 1 batch id 88161 loss 1.3836891651153564 train acc 0.4972105366885584\n",
      "epoch 1 batch id 88541 loss 1.5363901853561401 train acc 0.49661650393602963\n",
      "epoch 1 batch id 88921 loss 0.8420212864875793 train acc 0.49665240072648753\n",
      "epoch 1 batch id 89301 loss 0.9223138093948364 train acc 0.4969082793025834\n",
      "epoch 1 batch id 89681 loss 0.972545325756073 train acc 0.49719788054325886\n",
      "epoch 1 batch id 90061 loss 1.3273377418518066 train acc 0.49683148782491865\n",
      "epoch 1 batch id 90441 loss 1.3211524486541748 train acc 0.49625031788679913\n",
      "epoch 1 batch id 90821 loss 1.3960161209106445 train acc 0.49569001112077604\n",
      "epoch 1 batch id 91201 loss 1.347414255142212 train acc 0.49512769185644895\n",
      "epoch 1 batch id 91581 loss 1.4858896732330322 train acc 0.4945587785676068\n",
      "epoch 1 batch id 91961 loss 1.400120735168457 train acc 0.4939826733615337\n",
      "epoch 1 batch id 92341 loss 1.4155361652374268 train acc 0.4934245080733369\n",
      "epoch 1 batch id 92721 loss 1.304378628730774 train acc 0.49288220845331693\n",
      "epoch 1 batch id 93101 loss 1.41416335105896 train acc 0.49231916144831955\n",
      "epoch 1 batch id 93481 loss 1.3742287158966064 train acc 0.49177021934938653\n",
      "epoch 1 batch id 93861 loss 1.4218474626541138 train acc 0.49124802900033027\n",
      "epoch 1 batch id 94241 loss 1.3120349645614624 train acc 0.4907021957003852\n",
      "epoch 1 batch id 94621 loss 1.4507414102554321 train acc 0.49019955268914933\n",
      "epoch 1 batch id 95001 loss 1.3069566488265991 train acc 0.48972132530183893\n",
      "epoch 1 batch id 95381 loss 1.3104604482650757 train acc 0.48926492828760443\n",
      "epoch 1 batch id 95761 loss 1.4178954362869263 train acc 0.48878017277388497\n",
      "epoch 1 batch id 96141 loss 1.2180720567703247 train acc 0.4883392296210774\n",
      "epoch 1 batch id 96521 loss 1.2999495267868042 train acc 0.48791859414013533\n",
      "epoch 1 batch id 96901 loss 1.3354092836380005 train acc 0.48751609245518623\n",
      "epoch 1 batch id 97281 loss 1.3563860654830933 train acc 0.4871064557827325\n",
      "epoch 1 batch id 97661 loss 1.3395856618881226 train acc 0.48671536616458977\n",
      "epoch 1 batch id 98041 loss 1.3750715255737305 train acc 0.48636922308013997\n",
      "epoch 1 batch id 98421 loss 1.3847551345825195 train acc 0.4859876512634499\n",
      "epoch 1 batch id 98801 loss 1.3234734535217285 train acc 0.4856162893088127\n",
      "epoch 1 batch id 99181 loss 1.221563696861267 train acc 0.48527739057884073\n",
      "epoch 1 batch id 99561 loss 1.5207000970840454 train acc 0.48492773902431674\n",
      "epoch 1 batch id 99941 loss 1.323123812675476 train acc 0.4845868437378053\n",
      "epoch 1 batch id 100321 loss 1.2650630474090576 train acc 0.4842500884660241\n",
      "epoch 1 batch id 100701 loss 1.2582696676254272 train acc 0.4839017549478158\n",
      "epoch 1 batch id 101081 loss 1.411844253540039 train acc 0.4835787635658531\n",
      "epoch 1 batch id 101461 loss 1.405374526977539 train acc 0.4832528015690758\n",
      "epoch 1 batch id 101841 loss 1.3241757154464722 train acc 0.48292298165768205\n",
      "epoch 1 batch id 102221 loss 1.248659372329712 train acc 0.48262114071472595\n",
      "epoch 1 batch id 102601 loss 1.383408546447754 train acc 0.48231285513786415\n",
      "epoch 1 batch id 102981 loss 1.2884258031845093 train acc 0.48199045819131686\n",
      "epoch 1 batch id 103361 loss 1.2792298793792725 train acc 0.4816922001528623\n",
      "epoch 1 batch id 103741 loss 1.2090204954147339 train acc 0.48137820389238584\n",
      "epoch 1 batch id 104121 loss 1.422825813293457 train acc 0.4810499923166316\n",
      "epoch 1 batch id 104501 loss 1.2678638696670532 train acc 0.4807518289298667\n",
      "epoch 1 batch id 104881 loss 1.2054543495178223 train acc 0.48043720383100846\n",
      "epoch 1 batch id 105261 loss 1.2879213094711304 train acc 0.48014726489393034\n",
      "epoch 1 batch id 105641 loss 1.3802521228790283 train acc 0.4798613346144016\n",
      "epoch 1 batch id 106021 loss 1.2894363403320312 train acc 0.4795628637251111\n",
      "epoch 1 batch id 106401 loss 1.3455878496170044 train acc 0.4792762168588641\n",
      "epoch 1 batch id 106781 loss 1.1473743915557861 train acc 0.47899190281979004\n",
      "epoch 1 batch id 107161 loss 1.3501592874526978 train acc 0.4787266647847631\n",
      "epoch 1 batch id 107541 loss 1.2965244054794312 train acc 0.47845167773221375\n",
      "epoch 1 batch id 107921 loss 1.3507472276687622 train acc 0.4781838393361811\n",
      "epoch 1 batch id 108301 loss 1.317085862159729 train acc 0.4779242285389793\n",
      "epoch 1 batch id 108681 loss 1.4472148418426514 train acc 0.47767448427047965\n",
      "epoch 1 batch id 109061 loss 1.3675642013549805 train acc 0.4774482571680069\n",
      "epoch 1 batch id 109441 loss 1.3619698286056519 train acc 0.4772345944390128\n",
      "epoch 1 batch id 109821 loss 1.1788616180419922 train acc 0.47704802018739584\n",
      "epoch 1 batch id 110201 loss 1.3280359506607056 train acc 0.4768930749267248\n",
      "epoch 1 batch id 110581 loss 1.164515733718872 train acc 0.47679048615946684\n",
      "epoch 1 batch id 110961 loss 1.3347904682159424 train acc 0.4767004285289426\n",
      "epoch 1 batch id 111341 loss 1.0785259008407593 train acc 0.4766344215069022\n",
      "epoch 1 batch id 111721 loss 1.1855249404907227 train acc 0.4765683040789109\n",
      "epoch 1 batch id 112101 loss 1.0646358728408813 train acc 0.47651699137385034\n",
      "epoch 1 batch id 112481 loss 1.3749160766601562 train acc 0.47648311159218\n",
      "epoch 1 batch id 112861 loss 1.2268316745758057 train acc 0.476449736844437\n",
      "epoch 1 batch id 113241 loss 1.1120898723602295 train acc 0.4764101010234809\n",
      "epoch 1 batch id 113621 loss 1.2309497594833374 train acc 0.47637375573177493\n",
      "epoch 1 batch id 114001 loss 1.3155767917633057 train acc 0.47632723616459505\n",
      "epoch 1 batch id 114381 loss 1.3251475095748901 train acc 0.4763259686923528\n",
      "epoch 1 batch id 114761 loss 1.167614459991455 train acc 0.47630401442998926\n",
      "epoch 1 batch id 115141 loss 1.2037807703018188 train acc 0.476315995171138\n",
      "epoch 1 batch id 115521 loss 1.157698392868042 train acc 0.4763092316548506\n",
      "epoch 1 batch id 115901 loss 1.1897623538970947 train acc 0.47631720714230247\n",
      "epoch 1 batch id 116281 loss 1.0643922090530396 train acc 0.4763267429760666\n",
      "epoch 1 batch id 116661 loss 1.1414048671722412 train acc 0.47635336037750403\n",
      "epoch 1 batch id 117041 loss 1.1060771942138672 train acc 0.47637993844037557\n",
      "epoch 1 batch id 117421 loss 1.0944921970367432 train acc 0.47640993731955955\n",
      "epoch 1 batch id 117801 loss 1.2853388786315918 train acc 0.4764409364097079\n",
      "epoch 1 train acc 0.47646153626002746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123676/1621645026.py:27: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e6584d02bf485187e6c1d8c08c9af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.3608245073265043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf06f0a6f7644ea69287358bde0b7ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117926 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 1.1562622785568237 train acc 0.484375\n",
      "epoch 2 batch id 381 loss 1.12836754322052 train acc 0.4896653543307087\n",
      "epoch 2 batch id 761 loss 1.1433696746826172 train acc 0.49267000657030224\n",
      "epoch 2 batch id 1141 loss 1.134899377822876 train acc 0.4927695004382121\n",
      "epoch 2 batch id 1521 loss 1.3419753313064575 train acc 0.49414447731755423\n",
      "epoch 2 batch id 1901 loss 1.0554574728012085 train acc 0.4949944108364019\n",
      "epoch 2 batch id 2281 loss 1.0399073362350464 train acc 0.4958694103463393\n",
      "epoch 2 batch id 2661 loss 1.2018238306045532 train acc 0.4965179913566328\n",
      "epoch 2 batch id 3041 loss 1.049871802330017 train acc 0.49747718678066427\n",
      "epoch 2 batch id 3421 loss 1.1419020891189575 train acc 0.4977802543116048\n",
      "epoch 2 batch id 3801 loss 1.2575621604919434 train acc 0.4980145027624309\n",
      "epoch 2 batch id 4181 loss 1.4580129384994507 train acc 0.4987181595312126\n",
      "epoch 2 batch id 4561 loss 0.9631804823875427 train acc 0.4991675345319009\n",
      "epoch 2 batch id 4941 loss 1.0629570484161377 train acc 0.49984188423396075\n",
      "epoch 2 batch id 5321 loss 1.039676308631897 train acc 0.5002672195076113\n",
      "epoch 2 batch id 5701 loss 1.280289888381958 train acc 0.5005591124364147\n",
      "epoch 2 batch id 6081 loss 1.1175566911697388 train acc 0.5009352902483144\n",
      "epoch 2 batch id 6461 loss 0.9875781536102295 train acc 0.5013857181550844\n",
      "epoch 2 batch id 6841 loss 0.9991392493247986 train acc 0.5019025909954685\n",
      "epoch 2 batch id 7221 loss 1.0998787879943848 train acc 0.5021681553801413\n",
      "epoch 2 batch id 7601 loss 1.1267110109329224 train acc 0.5026394553348243\n",
      "epoch 2 batch id 7981 loss 1.0543644428253174 train acc 0.5030560863300338\n",
      "epoch 2 batch id 8361 loss 0.9951859712600708 train acc 0.5033993391938764\n",
      "epoch 2 batch id 8741 loss 0.9560497403144836 train acc 0.5034857281775541\n",
      "epoch 2 batch id 9121 loss 0.9782962203025818 train acc 0.5038253069838834\n",
      "epoch 2 batch id 9501 loss 1.0041612386703491 train acc 0.5038581465108936\n",
      "epoch 2 batch id 9881 loss 1.0793653726577759 train acc 0.5043233225382047\n",
      "epoch 2 batch id 10261 loss 0.9792816042900085 train acc 0.5044373111782477\n",
      "epoch 2 batch id 10641 loss 1.0682530403137207 train acc 0.5047061483883094\n",
      "epoch 2 batch id 11021 loss 1.0537680387496948 train acc 0.5050074857091008\n",
      "epoch 2 batch id 11401 loss 1.1545881032943726 train acc 0.5051928010700816\n",
      "epoch 2 batch id 11781 loss 1.0119324922561646 train acc 0.5053409621424327\n",
      "epoch 2 batch id 12161 loss 1.1163369417190552 train acc 0.5053847853794918\n",
      "epoch 2 batch id 12541 loss 1.1690338850021362 train acc 0.5055331014273184\n",
      "epoch 2 batch id 12921 loss 1.1432890892028809 train acc 0.505828689729897\n",
      "epoch 2 batch id 13301 loss 1.1081990003585815 train acc 0.5059946150665363\n",
      "epoch 2 batch id 13681 loss 1.0431370735168457 train acc 0.5060074190483151\n",
      "epoch 2 batch id 14061 loss 1.0812501907348633 train acc 0.5061006507360785\n",
      "epoch 2 batch id 14441 loss 1.104607105255127 train acc 0.5062841908455094\n",
      "epoch 2 batch id 14821 loss 1.0588513612747192 train acc 0.5064129866405775\n",
      "epoch 2 batch id 15201 loss 1.0938372611999512 train acc 0.5063472386685086\n",
      "epoch 2 batch id 15581 loss 1.0627162456512451 train acc 0.5064732286117708\n",
      "epoch 2 batch id 15961 loss 1.1341681480407715 train acc 0.5065168614121922\n",
      "epoch 2 batch id 16341 loss 1.0248632431030273 train acc 0.5066177483018175\n",
      "epoch 2 batch id 16721 loss 1.0349104404449463 train acc 0.5066888194485976\n",
      "epoch 2 batch id 17101 loss 1.2283886671066284 train acc 0.5067530773054207\n",
      "epoch 2 batch id 17481 loss 1.0180882215499878 train acc 0.5068494007779876\n",
      "epoch 2 batch id 17861 loss 1.2080215215682983 train acc 0.5067815351883993\n",
      "epoch 2 batch id 18241 loss 0.9433984756469727 train acc 0.5069537717230415\n",
      "epoch 2 batch id 18621 loss 1.063189148902893 train acc 0.5069352142742065\n",
      "epoch 2 batch id 19001 loss 1.0282796621322632 train acc 0.5069338455870743\n",
      "epoch 2 batch id 19381 loss 1.204923391342163 train acc 0.5069720344667458\n",
      "epoch 2 batch id 19761 loss 0.9538210034370422 train acc 0.5070577779464602\n",
      "epoch 2 batch id 20141 loss 1.0200146436691284 train acc 0.5071441648875428\n",
      "epoch 2 batch id 20521 loss 1.1252330541610718 train acc 0.5072585704887677\n",
      "epoch 2 batch id 20901 loss 1.1478004455566406 train acc 0.507319476340845\n",
      "epoch 2 batch id 21281 loss 0.9993033409118652 train acc 0.5073848150932757\n",
      "epoch 2 batch id 21661 loss 0.9163047075271606 train acc 0.5075365864918517\n",
      "epoch 2 batch id 22041 loss 1.0959802865982056 train acc 0.5075860044916293\n",
      "epoch 2 batch id 22421 loss 1.018481731414795 train acc 0.5076706826189733\n",
      "epoch 2 batch id 22801 loss 1.003818392753601 train acc 0.5077915990526731\n",
      "epoch 2 batch id 23181 loss 1.103703260421753 train acc 0.5078202514990725\n",
      "epoch 2 batch id 23561 loss 1.096196174621582 train acc 0.507827421374305\n",
      "epoch 2 batch id 23941 loss 0.9931740760803223 train acc 0.5078421953970177\n",
      "epoch 2 batch id 24321 loss 1.235901951789856 train acc 0.5078879877472143\n",
      "epoch 2 batch id 24701 loss 0.9658687710762024 train acc 0.5079171895874661\n",
      "epoch 2 batch id 25081 loss 1.0877903699874878 train acc 0.5079529823372274\n",
      "epoch 2 batch id 25461 loss 1.2662726640701294 train acc 0.5080343466478143\n",
      "epoch 2 batch id 25841 loss 1.1250288486480713 train acc 0.5081840631167525\n",
      "epoch 2 batch id 26221 loss 0.9075921773910522 train acc 0.5083246729720453\n",
      "epoch 2 batch id 26601 loss 1.0308598279953003 train acc 0.5083813813390474\n",
      "epoch 2 batch id 26981 loss 0.9724686741828918 train acc 0.5085077230273155\n",
      "epoch 2 batch id 27361 loss 1.0752049684524536 train acc 0.5085557454040422\n",
      "epoch 2 batch id 27741 loss 1.00558602809906 train acc 0.5086294879420352\n",
      "epoch 2 batch id 28121 loss 1.1658787727355957 train acc 0.508694569894385\n",
      "epoch 2 batch id 28501 loss 1.1942561864852905 train acc 0.5087601092944107\n",
      "epoch 2 batch id 28881 loss 1.0467925071716309 train acc 0.5087281647103632\n",
      "epoch 2 batch id 29261 loss 1.0315216779708862 train acc 0.5088209348621031\n",
      "epoch 2 batch id 29641 loss 0.9646214842796326 train acc 0.5088907678553355\n",
      "epoch 2 batch id 30021 loss 1.0760098695755005 train acc 0.5089447803204423\n",
      "epoch 2 batch id 30401 loss 1.269646406173706 train acc 0.5089768839840795\n",
      "epoch 2 batch id 30781 loss 1.0691726207733154 train acc 0.5089899207303207\n",
      "epoch 2 batch id 31161 loss 1.018328070640564 train acc 0.5090186852154938\n",
      "epoch 2 batch id 31541 loss 0.9586145877838135 train acc 0.509069049015567\n",
      "epoch 2 batch id 31921 loss 1.0865938663482666 train acc 0.509141709219636\n",
      "epoch 2 batch id 32301 loss 1.1223796606063843 train acc 0.5091507422370825\n",
      "epoch 2 batch id 32681 loss 0.932172417640686 train acc 0.5091643462562345\n",
      "epoch 2 batch id 33061 loss 1.0180730819702148 train acc 0.5092003228879949\n",
      "epoch 2 batch id 33441 loss 0.9952111840248108 train acc 0.5091985698693221\n",
      "epoch 2 batch id 33821 loss 0.9178944230079651 train acc 0.5092107159752817\n",
      "epoch 2 batch id 34201 loss 1.0942660570144653 train acc 0.5092239627496272\n",
      "epoch 2 batch id 34581 loss 1.0984715223312378 train acc 0.5093376782047946\n",
      "epoch 2 batch id 34961 loss 1.2016005516052246 train acc 0.5093662402391236\n",
      "epoch 2 batch id 35341 loss 1.0177724361419678 train acc 0.5093915353272404\n",
      "epoch 2 batch id 35721 loss 1.0751290321350098 train acc 0.5095217099185353\n",
      "epoch 2 batch id 36101 loss 1.2429488897323608 train acc 0.5095738345198194\n",
      "epoch 2 batch id 36481 loss 1.0366134643554688 train acc 0.5095674803322278\n",
      "epoch 2 batch id 36861 loss 1.0408196449279785 train acc 0.5096121239250156\n",
      "epoch 2 batch id 37241 loss 1.0443682670593262 train acc 0.509641591256948\n",
      "epoch 2 batch id 37621 loss 1.1693726778030396 train acc 0.5097402381648547\n",
      "epoch 2 batch id 38001 loss 0.9284136891365051 train acc 0.5098060741822583\n",
      "epoch 2 batch id 38381 loss 0.9870656728744507 train acc 0.5097806369036763\n",
      "epoch 2 batch id 38761 loss 1.0350353717803955 train acc 0.5098419642166094\n",
      "epoch 2 batch id 39141 loss 1.0468697547912598 train acc 0.5098949151784574\n",
      "epoch 2 batch id 39521 loss 1.2208987474441528 train acc 0.5099021722628476\n",
      "epoch 2 batch id 39901 loss 0.8860903978347778 train acc 0.5099637227137165\n",
      "epoch 2 batch id 40281 loss 1.0136573314666748 train acc 0.5099562293637199\n",
      "epoch 2 batch id 40661 loss 1.0310657024383545 train acc 0.5099630942426404\n",
      "epoch 2 batch id 41041 loss 1.0934323072433472 train acc 0.5099995279111132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 41421 loss 1.146718144416809 train acc 0.5100658482412302\n",
      "epoch 2 batch id 41801 loss 0.9659936428070068 train acc 0.5100902191335136\n",
      "epoch 2 batch id 42181 loss 0.947476327419281 train acc 0.5101111875014817\n",
      "epoch 2 batch id 42561 loss 1.7349556684494019 train acc 0.5101068172740302\n",
      "epoch 2 batch id 42941 loss 1.0640398263931274 train acc 0.5100941553526932\n",
      "epoch 2 batch id 43321 loss 1.149359107017517 train acc 0.5101401456568407\n",
      "epoch 2 batch id 43701 loss 1.1226372718811035 train acc 0.5101263415024828\n",
      "epoch 2 batch id 44081 loss 1.0013885498046875 train acc 0.5101602731335496\n",
      "epoch 2 batch id 44461 loss 0.9762449264526367 train acc 0.5102213878455275\n",
      "epoch 2 batch id 44841 loss 1.2651336193084717 train acc 0.5102992378626703\n",
      "epoch 2 batch id 45221 loss 1.1731750965118408 train acc 0.5103236051834324\n",
      "epoch 2 batch id 45601 loss 1.0288615226745605 train acc 0.5103359164272714\n",
      "epoch 2 batch id 45981 loss 1.1708037853240967 train acc 0.5103442862269199\n",
      "epoch 2 batch id 46361 loss 1.0617938041687012 train acc 0.5104111618601842\n",
      "epoch 2 batch id 46741 loss 1.2492613792419434 train acc 0.5104829673092146\n",
      "epoch 2 batch id 47121 loss 1.0434069633483887 train acc 0.51048795919017\n",
      "epoch 2 batch id 47501 loss 0.924517035484314 train acc 0.5105122786888697\n",
      "epoch 2 batch id 47881 loss 1.1062732934951782 train acc 0.5105642765397548\n",
      "epoch 2 batch id 48261 loss 0.9361218810081482 train acc 0.5105549123515882\n",
      "epoch 2 batch id 48641 loss 0.8774649500846863 train acc 0.5105444095516128\n",
      "epoch 2 batch id 49021 loss 1.0111234188079834 train acc 0.5105974990310275\n",
      "epoch 2 batch id 49401 loss 1.0350630283355713 train acc 0.5106092867553288\n",
      "epoch 2 batch id 49781 loss 0.9216309785842896 train acc 0.5106516542455957\n",
      "epoch 2 batch id 50161 loss 1.073872685432434 train acc 0.5107176765814079\n",
      "epoch 2 batch id 50541 loss 0.9211065173149109 train acc 0.5107388061178053\n",
      "epoch 2 batch id 50921 loss 1.1531282663345337 train acc 0.5107310834429803\n",
      "epoch 2 batch id 51301 loss 1.1033786535263062 train acc 0.5107545418217968\n",
      "epoch 2 batch id 51681 loss 1.151146650314331 train acc 0.5107728178634314\n",
      "epoch 2 batch id 52061 loss 1.118609070777893 train acc 0.5108094350857648\n",
      "epoch 2 batch id 52441 loss 1.062068223953247 train acc 0.5108645906828626\n",
      "epoch 2 batch id 52821 loss 0.9347590208053589 train acc 0.5108816805815869\n",
      "epoch 2 batch id 53201 loss 1.1858619451522827 train acc 0.5109035192007669\n",
      "epoch 2 batch id 53581 loss 0.9532578587532043 train acc 0.5109075511841884\n",
      "epoch 2 batch id 53961 loss 1.0574239492416382 train acc 0.5109367181853561\n",
      "epoch 2 batch id 54341 loss 1.0525447130203247 train acc 0.5109669149445171\n",
      "epoch 2 batch id 54721 loss 1.1130666732788086 train acc 0.5110161089892363\n",
      "epoch 2 batch id 55101 loss 1.048190712928772 train acc 0.5110121640260612\n",
      "epoch 2 batch id 55481 loss 0.9533798694610596 train acc 0.5110375624087525\n",
      "epoch 2 batch id 55861 loss 1.0947433710098267 train acc 0.5110631746656881\n",
      "epoch 2 batch id 56241 loss 1.0857967138290405 train acc 0.5110751053501894\n",
      "epoch 2 batch id 56621 loss 1.0878651142120361 train acc 0.5111034333551155\n",
      "epoch 2 batch id 57001 loss 0.9352926015853882 train acc 0.5111590695777267\n",
      "epoch 2 batch id 57381 loss 0.9723504185676575 train acc 0.5111532454122445\n",
      "epoch 2 batch id 57761 loss 1.2060726881027222 train acc 0.5111910501895742\n",
      "epoch 2 batch id 58141 loss 1.004457712173462 train acc 0.511209548769371\n",
      "epoch 2 batch id 58521 loss 1.0840444564819336 train acc 0.5112390210351839\n",
      "epoch 2 batch id 58901 loss 1.1371066570281982 train acc 0.511246360418329\n",
      "epoch 2 batch id 59281 loss 0.8602683544158936 train acc 0.5113110650967426\n",
      "epoch 2 batch id 59661 loss 1.00985848903656 train acc 0.5113262327986456\n",
      "epoch 2 batch id 60041 loss 1.1060181856155396 train acc 0.5113654107193418\n",
      "epoch 2 batch id 60421 loss 1.1258732080459595 train acc 0.5114154743383923\n",
      "epoch 2 batch id 60801 loss 1.089877963066101 train acc 0.5114507779477311\n",
      "epoch 2 batch id 61181 loss 0.9851142764091492 train acc 0.5115065849691898\n",
      "epoch 2 batch id 61561 loss 1.0902765989303589 train acc 0.5115330221243969\n",
      "epoch 2 batch id 61941 loss 1.1474425792694092 train acc 0.5115470266059637\n",
      "epoch 2 batch id 62321 loss 0.9696418046951294 train acc 0.5115613617400234\n",
      "epoch 2 batch id 62701 loss 1.052441120147705 train acc 0.5115326609623451\n",
      "epoch 2 batch id 63081 loss 1.0822093486785889 train acc 0.5115362589369223\n",
      "epoch 2 batch id 63461 loss 1.0202720165252686 train acc 0.5115809315957832\n",
      "epoch 2 batch id 63841 loss 0.9621810913085938 train acc 0.5116304569164016\n",
      "epoch 2 batch id 64221 loss 1.0307610034942627 train acc 0.5116643115180393\n",
      "epoch 2 batch id 64601 loss 1.039155125617981 train acc 0.511671887819074\n",
      "epoch 2 batch id 64981 loss 1.0245299339294434 train acc 0.5116702382234807\n",
      "epoch 2 batch id 65361 loss 1.0767821073532104 train acc 0.511722634675112\n",
      "epoch 2 batch id 65741 loss 0.917761504650116 train acc 0.5117211861699701\n",
      "epoch 2 batch id 66121 loss 1.1308786869049072 train acc 0.5117500018904735\n",
      "epoch 2 batch id 66501 loss 1.0548925399780273 train acc 0.5117697948151155\n",
      "epoch 2 batch id 66881 loss 1.0771130323410034 train acc 0.5117968387882956\n",
      "epoch 2 batch id 67261 loss 0.9496083855628967 train acc 0.5118523828816104\n",
      "epoch 2 batch id 67641 loss 1.2687294483184814 train acc 0.5118929809582945\n",
      "epoch 2 batch id 68021 loss 1.173779845237732 train acc 0.5119014256626631\n",
      "epoch 2 batch id 68401 loss 1.1298549175262451 train acc 0.5119243962076577\n",
      "epoch 2 batch id 68781 loss 0.9633341431617737 train acc 0.5119539280469897\n",
      "epoch 2 batch id 69161 loss 1.035828948020935 train acc 0.5119731947918624\n",
      "epoch 2 batch id 69541 loss 1.0464410781860352 train acc 0.5119798931565551\n",
      "epoch 2 batch id 69921 loss 1.0560004711151123 train acc 0.512004619499149\n",
      "epoch 2 batch id 70301 loss 0.9420773983001709 train acc 0.5120290785337335\n",
      "epoch 2 batch id 70681 loss 0.9381113052368164 train acc 0.5120756020005376\n",
      "epoch 2 batch id 71061 loss 1.0349313020706177 train acc 0.5120646785860036\n",
      "epoch 2 batch id 71441 loss 1.1993685960769653 train acc 0.5120683063646926\n",
      "epoch 2 batch id 71821 loss 0.993036687374115 train acc 0.5121064869606382\n",
      "epoch 2 batch id 72201 loss 0.9885423183441162 train acc 0.5121263036523039\n",
      "epoch 2 batch id 72581 loss 0.9392145872116089 train acc 0.5121711002190656\n",
      "epoch 2 batch id 72961 loss 0.9358065724372864 train acc 0.5122017242088239\n",
      "epoch 2 batch id 73341 loss 1.0103967189788818 train acc 0.5122235090195116\n",
      "epoch 2 batch id 73721 loss 1.1490554809570312 train acc 0.5122295970618955\n",
      "epoch 2 batch id 74101 loss 1.1836497783660889 train acc 0.5122790599991903\n",
      "epoch 2 batch id 74481 loss 0.9233896732330322 train acc 0.5123244518736322\n",
      "epoch 2 batch id 74861 loss 1.0406335592269897 train acc 0.5123556073923672\n",
      "epoch 2 batch id 75241 loss 0.9906141757965088 train acc 0.5123673429380258\n",
      "epoch 2 batch id 75621 loss 0.9985413551330566 train acc 0.5124122267624073\n",
      "epoch 2 batch id 76001 loss 1.0274497270584106 train acc 0.5124198612518256\n",
      "epoch 2 batch id 76381 loss 1.0316078662872314 train acc 0.512447262735497\n",
      "epoch 2 batch id 76761 loss 1.1365365982055664 train acc 0.5125014655879939\n",
      "epoch 2 batch id 77141 loss 1.103954553604126 train acc 0.5125209032810049\n",
      "epoch 2 batch id 77521 loss 0.9941972494125366 train acc 0.5125431737851679\n",
      "epoch 2 batch id 77901 loss 1.0957281589508057 train acc 0.5125730494473755\n",
      "epoch 2 batch id 78281 loss 0.9294867515563965 train acc 0.5126046310726741\n",
      "epoch 2 batch id 78661 loss 1.160955786705017 train acc 0.5126309416356263\n",
      "epoch 2 batch id 79041 loss 1.1390984058380127 train acc 0.512630114434281\n",
      "epoch 2 batch id 79421 loss 1.102981686592102 train acc 0.512622015902595\n",
      "epoch 2 batch id 79801 loss 1.0203392505645752 train acc 0.5126564830014662\n",
      "epoch 2 batch id 80181 loss 1.0639495849609375 train acc 0.5126656798368691\n",
      "epoch 2 batch id 80561 loss 1.0387868881225586 train acc 0.5126670318144015\n",
      "epoch 2 batch id 80941 loss 1.0005226135253906 train acc 0.5126907639515202\n",
      "epoch 2 batch id 81321 loss 1.0164260864257812 train acc 0.5126962131552736\n",
      "epoch 2 batch id 81701 loss 1.049744963645935 train acc 0.5127345060035985\n",
      "epoch 2 batch id 82081 loss 1.1284070014953613 train acc 0.512747507035733\n",
      "epoch 2 batch id 82461 loss 1.004377007484436 train acc 0.5127660727495422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 82841 loss 0.9468023180961609 train acc 0.5127839022947575\n",
      "epoch 2 batch id 83221 loss 1.035330057144165 train acc 0.5128060750892203\n",
      "epoch 2 batch id 83601 loss 1.0053915977478027 train acc 0.51284131619239\n",
      "epoch 2 batch id 83981 loss 0.9899635314941406 train acc 0.5128689822697992\n",
      "epoch 2 batch id 84361 loss 1.0592459440231323 train acc 0.5128812114010028\n",
      "epoch 2 batch id 84741 loss 1.1330838203430176 train acc 0.512873970392136\n",
      "epoch 2 batch id 85121 loss 1.0495489835739136 train acc 0.5128943283678529\n",
      "epoch 2 batch id 85501 loss 1.0428295135498047 train acc 0.5128971444778424\n",
      "epoch 2 batch id 85881 loss 1.092374563217163 train acc 0.5129421452358496\n",
      "epoch 2 batch id 86261 loss 1.0346606969833374 train acc 0.5129539638422925\n",
      "epoch 2 batch id 86641 loss 1.042328119277954 train acc 0.5129813685206773\n",
      "epoch 2 batch id 87021 loss 0.9245150685310364 train acc 0.5130018903483067\n",
      "epoch 2 batch id 87401 loss 1.0028098821640015 train acc 0.5130315299596114\n",
      "epoch 2 batch id 87781 loss 0.9325585961341858 train acc 0.5130669649468563\n",
      "epoch 2 batch id 88161 loss 0.9549875855445862 train acc 0.5130918149748755\n",
      "epoch 2 batch id 88541 loss 1.1091731786727905 train acc 0.5130936868230537\n",
      "epoch 2 batch id 88921 loss 0.9978502988815308 train acc 0.5131081943522903\n",
      "epoch 2 batch id 89301 loss 0.9809707999229431 train acc 0.5131021069192954\n",
      "epoch 2 batch id 89681 loss 1.1000449657440186 train acc 0.5131283033752969\n",
      "epoch 2 batch id 90061 loss 0.9451589584350586 train acc 0.513181864236462\n",
      "epoch 2 batch id 90441 loss 0.938991904258728 train acc 0.5132078509746686\n",
      "epoch 2 batch id 90821 loss 1.143356204032898 train acc 0.5132417062133207\n",
      "epoch 2 batch id 91201 loss 1.107119083404541 train acc 0.5132687689827963\n",
      "epoch 2 batch id 91581 loss 1.205936312675476 train acc 0.5132841760299626\n",
      "epoch 2 batch id 91961 loss 1.0693799257278442 train acc 0.5132909602983874\n",
      "epoch 2 batch id 92341 loss 1.1516014337539673 train acc 0.5133022573937904\n",
      "epoch 2 batch id 92721 loss 1.1104449033737183 train acc 0.5133126193095415\n",
      "epoch 2 batch id 93101 loss 0.9449686408042908 train acc 0.5133052746479629\n",
      "epoch 2 batch id 93481 loss 1.1164969205856323 train acc 0.5133158743487981\n",
      "epoch 2 batch id 93861 loss 1.0254220962524414 train acc 0.5133458651623145\n",
      "epoch 2 batch id 94241 loss 1.117322325706482 train acc 0.5133643398308592\n",
      "epoch 2 batch id 94621 loss 1.034080982208252 train acc 0.5133785377981632\n",
      "epoch 2 batch id 95001 loss 0.9869956970214844 train acc 0.5134085759623583\n",
      "epoch 2 batch id 95381 loss 1.011499047279358 train acc 0.5134390300479131\n",
      "epoch 2 batch id 95761 loss 1.071060299873352 train acc 0.5134511309405708\n",
      "epoch 2 batch id 96141 loss 0.9242273569107056 train acc 0.5134665491309639\n",
      "epoch 2 batch id 96521 loss 1.000989556312561 train acc 0.5135069376094321\n",
      "epoch 2 batch id 96901 loss 1.0546561479568481 train acc 0.5135142761684606\n",
      "epoch 2 batch id 97281 loss 1.1124153137207031 train acc 0.5135302307233683\n",
      "epoch 2 batch id 97661 loss 1.03175950050354 train acc 0.5135601404347693\n",
      "epoch 2 batch id 98041 loss 0.9860305190086365 train acc 0.5136081460817413\n",
      "epoch 2 batch id 98421 loss 1.159659504890442 train acc 0.5136138692453847\n",
      "epoch 2 batch id 98801 loss 1.0022228956222534 train acc 0.513632041932774\n",
      "epoch 2 batch id 99181 loss 0.9974846243858337 train acc 0.5136626785876327\n",
      "epoch 2 batch id 99561 loss 1.1574585437774658 train acc 0.5136772305420797\n",
      "epoch 2 batch id 99941 loss 0.9835082292556763 train acc 0.5137137160924946\n",
      "epoch 2 batch id 100321 loss 1.0548442602157593 train acc 0.5137394899871413\n",
      "epoch 2 batch id 100701 loss 0.9266039133071899 train acc 0.5137714310185599\n",
      "epoch 2 batch id 101081 loss 1.034464955329895 train acc 0.5137916930481495\n",
      "epoch 2 batch id 101461 loss 1.069832444190979 train acc 0.513812419303969\n",
      "epoch 2 batch id 101841 loss 1.0660784244537354 train acc 0.5138285415500633\n",
      "epoch 2 batch id 102221 loss 0.9976302981376648 train acc 0.5138757263673804\n",
      "epoch 2 batch id 102601 loss 1.0757163763046265 train acc 0.5138954542353389\n",
      "epoch 2 batch id 102981 loss 1.0670591592788696 train acc 0.5139197400491353\n",
      "epoch 2 batch id 103361 loss 0.9654636979103088 train acc 0.5139470218457639\n",
      "epoch 2 batch id 103741 loss 0.8569368124008179 train acc 0.5139599459230199\n",
      "epoch 2 batch id 104121 loss 1.1809790134429932 train acc 0.5139616707964771\n",
      "epoch 2 batch id 104501 loss 0.9901665449142456 train acc 0.5139786341757495\n",
      "epoch 2 batch id 104881 loss 0.8938557505607605 train acc 0.5139723829864322\n",
      "epoch 2 batch id 105261 loss 1.056566834449768 train acc 0.5139871070481945\n",
      "epoch 2 batch id 105641 loss 1.1125502586364746 train acc 0.5139990628638502\n",
      "epoch 2 batch id 106021 loss 1.0918307304382324 train acc 0.514012259363711\n",
      "epoch 2 batch id 106401 loss 1.2518491744995117 train acc 0.5140219840509018\n",
      "epoch 2 batch id 106781 loss 0.9899752736091614 train acc 0.5140306152311741\n",
      "epoch 2 batch id 107161 loss 1.1259385347366333 train acc 0.5140550783400677\n",
      "epoch 2 batch id 107541 loss 0.9256062507629395 train acc 0.5140690527333761\n",
      "epoch 2 batch id 107921 loss 1.2061537504196167 train acc 0.5140848108801809\n",
      "epoch 2 batch id 108301 loss 1.0597801208496094 train acc 0.5140994485277144\n",
      "epoch 2 batch id 108681 loss 1.2885068655014038 train acc 0.5141126898905972\n",
      "epoch 2 batch id 109061 loss 1.0399389266967773 train acc 0.5141466129047048\n",
      "epoch 2 batch id 109441 loss 1.123673439025879 train acc 0.5141598840927989\n",
      "epoch 2 batch id 109821 loss 0.9991161227226257 train acc 0.5141854415366824\n",
      "epoch 2 batch id 110201 loss 1.1025253534317017 train acc 0.5142062855600221\n",
      "epoch 2 batch id 110581 loss 1.0317203998565674 train acc 0.5142231712500339\n",
      "epoch 2 batch id 110961 loss 1.0864439010620117 train acc 0.5142400821009183\n",
      "epoch 2 batch id 111341 loss 0.9100791215896606 train acc 0.5142647362606767\n",
      "epoch 2 batch id 111721 loss 1.1442115306854248 train acc 0.5142756565462178\n",
      "epoch 2 batch id 112101 loss 0.9450476169586182 train acc 0.5143025318685828\n",
      "epoch 2 batch id 112481 loss 1.3667182922363281 train acc 0.5143351988335808\n",
      "epoch 2 batch id 112861 loss 1.0193043947219849 train acc 0.5143654307067986\n",
      "epoch 2 batch id 113241 loss 0.9464994668960571 train acc 0.5143772463153805\n",
      "epoch 2 batch id 113621 loss 1.145263433456421 train acc 0.5143896704834494\n",
      "epoch 2 batch id 114001 loss 1.2175594568252563 train acc 0.5143924176103718\n",
      "epoch 2 batch id 114381 loss 1.209699273109436 train acc 0.5144291610931886\n",
      "epoch 2 batch id 114761 loss 0.9987596869468689 train acc 0.5144470083477837\n",
      "epoch 2 batch id 115141 loss 1.0481362342834473 train acc 0.5144736942097081\n",
      "epoch 2 batch id 115521 loss 1.0086590051651 train acc 0.5144835679227153\n",
      "epoch 2 batch id 115901 loss 1.1465433835983276 train acc 0.5145114418771193\n",
      "epoch 2 batch id 116281 loss 0.903880774974823 train acc 0.5145301306748308\n",
      "epoch 2 batch id 116661 loss 0.9747617244720459 train acc 0.5145567338270716\n",
      "epoch 2 batch id 117041 loss 0.9629279375076294 train acc 0.5145750207192351\n",
      "epoch 2 batch id 117421 loss 0.9635593891143799 train acc 0.5145868019774997\n",
      "epoch 2 batch id 117801 loss 1.1908049583435059 train acc 0.514600231534537\n",
      "epoch 2 train acc 0.5146162900886997\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02640332fd4543fc94c8312c80efda3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.20559843718200937\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2960a0f1f32f40c69d9a232c5b68a094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117926 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 1.1323870420455933 train acc 0.5\n",
      "epoch 3 batch id 381 loss 1.0522063970565796 train acc 0.5181266404199475\n",
      "epoch 3 batch id 761 loss 1.075239658355713 train acc 0.5202858081471747\n",
      "epoch 3 batch id 1141 loss 1.1320888996124268 train acc 0.5189526730937774\n",
      "epoch 3 batch id 1521 loss 1.1419003009796143 train acc 0.5199087771203156\n",
      "epoch 3 batch id 1901 loss 0.9633008241653442 train acc 0.5197182403997895\n",
      "epoch 3 batch id 2281 loss 1.0499897003173828 train acc 0.5199062911003945\n",
      "epoch 3 batch id 2661 loss 1.1834346055984497 train acc 0.519858605787298\n",
      "epoch 3 batch id 3041 loss 0.9525055885314941 train acc 0.5203520634659652\n",
      "epoch 3 batch id 3421 loss 1.0591604709625244 train acc 0.5202015127155802\n",
      "epoch 3 batch id 3801 loss 1.2669360637664795 train acc 0.519719317284925\n",
      "epoch 3 batch id 4181 loss 1.2200915813446045 train acc 0.5197209100693614\n",
      "epoch 3 batch id 4561 loss 0.9123470783233643 train acc 0.5196434444200834\n",
      "epoch 3 batch id 4941 loss 1.0299144983291626 train acc 0.5196095173041895\n",
      "epoch 3 batch id 5321 loss 1.017423152923584 train acc 0.5195745630520578\n",
      "epoch 3 batch id 5701 loss 1.1210732460021973 train acc 0.5195881205051746\n",
      "epoch 3 batch id 6081 loss 1.106400966644287 train acc 0.5196899153099819\n",
      "epoch 3 batch id 6461 loss 0.9434858560562134 train acc 0.5198837254295001\n",
      "epoch 3 batch id 6841 loss 1.0379340648651123 train acc 0.5199989036690542\n",
      "epoch 3 batch id 7221 loss 1.0389914512634277 train acc 0.5199548192771084\n",
      "epoch 3 batch id 7601 loss 1.0921965837478638 train acc 0.5200549269832917\n",
      "epoch 3 batch id 7981 loss 1.000183343887329 train acc 0.520102430773086\n",
      "epoch 3 batch id 8361 loss 0.9717391729354858 train acc 0.5202054180121995\n",
      "epoch 3 batch id 8741 loss 0.9819799065589905 train acc 0.5200724316439767\n",
      "epoch 3 batch id 9121 loss 0.936332643032074 train acc 0.520022475605745\n",
      "epoch 3 batch id 9501 loss 0.9319812655448914 train acc 0.5199650036838227\n",
      "epoch 3 batch id 9881 loss 1.076934576034546 train acc 0.520157056472017\n",
      "epoch 3 batch id 10261 loss 1.0073391199111938 train acc 0.5200927662995809\n",
      "epoch 3 batch id 10641 loss 1.1451352834701538 train acc 0.5201725636688281\n",
      "epoch 3 batch id 11021 loss 1.1258279085159302 train acc 0.5204084815352509\n",
      "epoch 3 batch id 11401 loss 1.0447970628738403 train acc 0.5204655841592842\n",
      "epoch 3 batch id 11781 loss 0.9558350443840027 train acc 0.520461972667855\n",
      "epoch 3 batch id 12161 loss 1.1113824844360352 train acc 0.5203635083463531\n",
      "epoch 3 batch id 12541 loss 1.1573091745376587 train acc 0.520283470217686\n",
      "epoch 3 batch id 12921 loss 1.1691962480545044 train acc 0.5203544617289684\n",
      "epoch 3 batch id 13301 loss 1.1092761754989624 train acc 0.5204331441245019\n",
      "epoch 3 batch id 13681 loss 1.015400767326355 train acc 0.5203270046049265\n",
      "epoch 3 batch id 14061 loss 1.0501797199249268 train acc 0.5203532821278715\n",
      "epoch 3 batch id 14441 loss 1.0430934429168701 train acc 0.5204041444498303\n",
      "epoch 3 batch id 14821 loss 1.0354703664779663 train acc 0.5204945685176439\n",
      "epoch 3 batch id 15201 loss 1.0440045595169067 train acc 0.5203841441352542\n",
      "epoch 3 batch id 15581 loss 1.0265929698944092 train acc 0.5204225097875618\n",
      "epoch 3 batch id 15961 loss 1.2034282684326172 train acc 0.5203640905958273\n",
      "epoch 3 batch id 16341 loss 1.0183963775634766 train acc 0.5202825714460559\n",
      "epoch 3 batch id 16721 loss 1.0841054916381836 train acc 0.5202570868967167\n",
      "epoch 3 batch id 17101 loss 1.2061214447021484 train acc 0.520188877843401\n",
      "epoch 3 batch id 17481 loss 0.9798409938812256 train acc 0.5202139108174589\n",
      "epoch 3 batch id 17861 loss 1.1357899904251099 train acc 0.5200821622529533\n",
      "epoch 3 batch id 18241 loss 1.0150142908096313 train acc 0.5201649100926484\n",
      "epoch 3 batch id 18621 loss 1.0309972763061523 train acc 0.5200924023951453\n",
      "epoch 3 batch id 19001 loss 1.0272718667984009 train acc 0.5200047036998052\n",
      "epoch 3 batch id 19381 loss 1.0546209812164307 train acc 0.5199252812032403\n",
      "epoch 3 batch id 19761 loss 0.9315099716186523 train acc 0.5199121691716007\n",
      "epoch 3 batch id 20141 loss 0.9509913325309753 train acc 0.5199507534382602\n",
      "epoch 3 batch id 20521 loss 1.1266506910324097 train acc 0.5199589749524877\n",
      "epoch 3 batch id 20901 loss 1.1017284393310547 train acc 0.5199467130759294\n",
      "epoch 3 batch id 21281 loss 0.921085000038147 train acc 0.5199165335275598\n",
      "epoch 3 batch id 21661 loss 0.9290299415588379 train acc 0.5199840727574904\n",
      "epoch 3 batch id 22041 loss 1.0407229661941528 train acc 0.5199521630143823\n",
      "epoch 3 batch id 22421 loss 0.958051860332489 train acc 0.5199547856919852\n",
      "epoch 3 batch id 22801 loss 0.9794249534606934 train acc 0.5200237928161046\n",
      "epoch 3 batch id 23181 loss 1.0101242065429688 train acc 0.519995497390104\n",
      "epoch 3 batch id 23561 loss 1.0142799615859985 train acc 0.5199449036543441\n",
      "epoch 3 batch id 23941 loss 1.0407068729400635 train acc 0.5198671995739527\n",
      "epoch 3 batch id 24321 loss 1.0492421388626099 train acc 0.5198805815961515\n",
      "epoch 3 batch id 24701 loss 0.9735406041145325 train acc 0.5198796354398607\n",
      "epoch 3 batch id 25081 loss 1.0861660242080688 train acc 0.5199241956062358\n",
      "epoch 3 batch id 25461 loss 1.148679256439209 train acc 0.5200208161501905\n",
      "epoch 3 batch id 25841 loss 1.1048916578292847 train acc 0.520162363105143\n",
      "epoch 3 batch id 26221 loss 0.9009843468666077 train acc 0.5202759715495213\n",
      "epoch 3 batch id 26601 loss 0.993218183517456 train acc 0.5203105616330214\n",
      "epoch 3 batch id 26981 loss 0.9095794558525085 train acc 0.5204530502946518\n",
      "epoch 3 batch id 27361 loss 0.9873340129852295 train acc 0.5205099183143891\n",
      "epoch 3 batch id 27741 loss 1.0770695209503174 train acc 0.5206136674957644\n",
      "epoch 3 batch id 28121 loss 1.0406086444854736 train acc 0.5206707176131716\n",
      "epoch 3 batch id 28501 loss 1.1242133378982544 train acc 0.5207569471246623\n",
      "epoch 3 batch id 28881 loss 1.0533891916275024 train acc 0.520741361102455\n",
      "epoch 3 batch id 29261 loss 0.9977850914001465 train acc 0.5208495309456272\n",
      "epoch 3 batch id 29641 loss 0.927070140838623 train acc 0.5209090659896765\n",
      "epoch 3 batch id 30021 loss 1.099661946296692 train acc 0.520986351220812\n",
      "epoch 3 batch id 30401 loss 1.1537214517593384 train acc 0.5210216152429197\n",
      "epoch 3 batch id 30781 loss 1.0488770008087158 train acc 0.5210610847600793\n",
      "epoch 3 batch id 31161 loss 0.9843606352806091 train acc 0.5211121273386605\n",
      "epoch 3 batch id 31541 loss 0.9470701217651367 train acc 0.5211649123363241\n",
      "epoch 3 batch id 31921 loss 1.0126829147338867 train acc 0.5212276988502866\n",
      "epoch 3 batch id 32301 loss 1.165822148323059 train acc 0.5212580492863998\n",
      "epoch 3 batch id 32681 loss 0.9059877991676331 train acc 0.5212475329702273\n",
      "epoch 3 batch id 33061 loss 1.0048357248306274 train acc 0.5213166570884123\n",
      "epoch 3 batch id 33441 loss 0.9779604077339172 train acc 0.5213337482431746\n",
      "epoch 3 batch id 33821 loss 0.8632944822311401 train acc 0.5213624671062358\n",
      "epoch 3 batch id 34201 loss 1.0590965747833252 train acc 0.5213800400573083\n",
      "epoch 3 batch id 34581 loss 1.031175136566162 train acc 0.5214907572077152\n",
      "epoch 3 batch id 34961 loss 1.1402586698532104 train acc 0.5215087883641772\n",
      "epoch 3 batch id 35341 loss 0.9882094860076904 train acc 0.5215578223592994\n",
      "epoch 3 batch id 35721 loss 1.0845309495925903 train acc 0.5216862979759805\n",
      "epoch 3 batch id 36101 loss 1.2756562232971191 train acc 0.5217540719093654\n",
      "epoch 3 batch id 36481 loss 0.9716020822525024 train acc 0.5217557598475919\n",
      "epoch 3 batch id 36861 loss 1.1604690551757812 train acc 0.5217964108407259\n",
      "epoch 3 batch id 37241 loss 1.034447431564331 train acc 0.5218383300663247\n",
      "epoch 3 batch id 37621 loss 1.2014814615249634 train acc 0.521935886871694\n",
      "epoch 3 batch id 38001 loss 0.9340416789054871 train acc 0.5220022992815978\n",
      "epoch 3 batch id 38381 loss 1.0149203538894653 train acc 0.5219953394908939\n",
      "epoch 3 batch id 38761 loss 0.9900079965591431 train acc 0.5220554326513764\n",
      "epoch 3 batch id 39141 loss 0.9968512654304504 train acc 0.5220939998978054\n",
      "epoch 3 batch id 39521 loss 1.1838299036026 train acc 0.5221061271222894\n",
      "epoch 3 batch id 39901 loss 0.8525790572166443 train acc 0.5221607071251347\n",
      "epoch 3 batch id 40281 loss 1.0045000314712524 train acc 0.5221483143417492\n",
      "epoch 3 batch id 40661 loss 0.9648049473762512 train acc 0.5221496028135068\n",
      "epoch 3 batch id 41041 loss 1.058136224746704 train acc 0.5221775176043468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 41421 loss 1.1318968534469604 train acc 0.5222437742932329\n",
      "epoch 3 batch id 41801 loss 0.933627724647522 train acc 0.5222710730604531\n",
      "epoch 3 batch id 42181 loss 0.8616572618484497 train acc 0.5222708387662691\n",
      "epoch 3 batch id 42561 loss 1.114953637123108 train acc 0.522300712506755\n",
      "epoch 3 batch id 42941 loss 1.0453059673309326 train acc 0.5222922148995133\n",
      "epoch 3 batch id 43321 loss 1.1099225282669067 train acc 0.5223170488908382\n",
      "epoch 3 batch id 43701 loss 1.0604658126831055 train acc 0.5223171380517608\n",
      "epoch 3 batch id 44081 loss 0.9828829169273376 train acc 0.5223462914861278\n",
      "epoch 3 batch id 44461 loss 0.9485629200935364 train acc 0.5223953296147185\n",
      "epoch 3 batch id 44841 loss 1.140298843383789 train acc 0.5224728066947659\n",
      "epoch 3 batch id 45221 loss 1.2266210317611694 train acc 0.5224950797195993\n",
      "epoch 3 batch id 45601 loss 0.9908553957939148 train acc 0.5225104712615951\n",
      "epoch 3 batch id 45981 loss 1.1445579528808594 train acc 0.5225252685891999\n",
      "epoch 3 batch id 46361 loss 1.0451490879058838 train acc 0.5225954331226678\n",
      "epoch 3 batch id 46741 loss 1.223232388496399 train acc 0.5226587738816029\n",
      "epoch 3 batch id 47121 loss 1.0103944540023804 train acc 0.522647810954776\n",
      "epoch 3 batch id 47501 loss 0.8717615008354187 train acc 0.5226564309172439\n",
      "epoch 3 batch id 47881 loss 1.0945899486541748 train acc 0.5227102739082308\n",
      "epoch 3 batch id 48261 loss 0.9095178842544556 train acc 0.522722475187004\n",
      "epoch 3 batch id 48641 loss 0.8648756742477417 train acc 0.5227078236467178\n",
      "epoch 3 batch id 49021 loss 0.9815284013748169 train acc 0.5227491789233186\n",
      "epoch 3 batch id 49401 loss 1.004183053970337 train acc 0.5227418220278942\n",
      "epoch 3 batch id 49781 loss 0.9430480599403381 train acc 0.5227822864144955\n",
      "epoch 3 batch id 50161 loss 1.0708317756652832 train acc 0.5228486149598294\n",
      "epoch 3 batch id 50541 loss 0.9010657668113708 train acc 0.5228598439880493\n",
      "epoch 3 batch id 50921 loss 1.0982980728149414 train acc 0.5228592452033543\n",
      "epoch 3 batch id 51301 loss 0.9466123580932617 train acc 0.5228827167111753\n",
      "epoch 3 batch id 51681 loss 1.140976071357727 train acc 0.522885586579207\n",
      "epoch 3 batch id 52061 loss 1.1137815713882446 train acc 0.5229118245903843\n",
      "epoch 3 batch id 52441 loss 1.0791913270950317 train acc 0.5229624125207376\n",
      "epoch 3 batch id 52821 loss 0.8980022668838501 train acc 0.5229735214214044\n",
      "epoch 3 batch id 53201 loss 1.1913678646087646 train acc 0.522987702298829\n",
      "epoch 3 batch id 53581 loss 0.9259108304977417 train acc 0.5229876845336966\n",
      "epoch 3 batch id 53961 loss 1.0054666996002197 train acc 0.5230117005800485\n",
      "epoch 3 batch id 54341 loss 1.0471501350402832 train acc 0.5230477447967464\n",
      "epoch 3 batch id 54721 loss 1.0551719665527344 train acc 0.5230892847352936\n",
      "epoch 3 batch id 55101 loss 1.0296854972839355 train acc 0.5230803433694489\n",
      "epoch 3 batch id 55481 loss 0.9439157843589783 train acc 0.5231033484436113\n",
      "epoch 3 batch id 55861 loss 1.065453290939331 train acc 0.5231165303163209\n",
      "epoch 3 batch id 56241 loss 1.0111453533172607 train acc 0.523118976814068\n",
      "epoch 3 batch id 56621 loss 1.0646088123321533 train acc 0.5231360162307271\n",
      "epoch 3 batch id 57001 loss 0.9524157643318176 train acc 0.5231802402589428\n",
      "epoch 3 batch id 57381 loss 1.0422042608261108 train acc 0.5231691457102525\n",
      "epoch 3 batch id 57761 loss 1.1881918907165527 train acc 0.5232120288776164\n",
      "epoch 3 batch id 58141 loss 0.9798005223274231 train acc 0.523226133451437\n",
      "epoch 3 batch id 58521 loss 1.113330602645874 train acc 0.5232499337844534\n",
      "epoch 3 batch id 58901 loss 1.1256550550460815 train acc 0.5232474300096772\n",
      "epoch 3 batch id 59281 loss 0.8647279739379883 train acc 0.5233100614024729\n",
      "epoch 3 batch id 59661 loss 0.9743221402168274 train acc 0.5233265868825531\n",
      "epoch 3 batch id 60041 loss 1.068559169769287 train acc 0.523364503006279\n",
      "epoch 3 batch id 60421 loss 1.1445554494857788 train acc 0.5234078900547823\n",
      "epoch 3 batch id 60801 loss 1.0729728937149048 train acc 0.5234324887748557\n",
      "epoch 3 batch id 61181 loss 0.9573550224304199 train acc 0.5234680190745493\n",
      "epoch 3 batch id 61561 loss 1.0828981399536133 train acc 0.5234820442325498\n",
      "epoch 3 batch id 61941 loss 1.093564748764038 train acc 0.5234863115706883\n",
      "epoch 3 batch id 62321 loss 0.9733538627624512 train acc 0.5234912790231222\n",
      "epoch 3 batch id 62701 loss 1.0378443002700806 train acc 0.5234610492655619\n",
      "epoch 3 batch id 63081 loss 1.102837085723877 train acc 0.523468338326913\n",
      "epoch 3 batch id 63461 loss 1.0026429891586304 train acc 0.5235082865854619\n",
      "epoch 3 batch id 63841 loss 0.9251534342765808 train acc 0.5235568149778356\n",
      "epoch 3 batch id 64221 loss 1.0090744495391846 train acc 0.5235782493265443\n",
      "epoch 3 batch id 64601 loss 1.0383878946304321 train acc 0.5235861286976982\n",
      "epoch 3 batch id 64981 loss 1.0074489116668701 train acc 0.5235770840707283\n",
      "epoch 3 batch id 65361 loss 1.0692447423934937 train acc 0.5236295822432337\n",
      "epoch 3 batch id 65741 loss 0.8809915781021118 train acc 0.5236256198567104\n",
      "epoch 3 batch id 66121 loss 1.0971325635910034 train acc 0.5236543136824912\n",
      "epoch 3 batch id 66501 loss 1.061746597290039 train acc 0.5236704617223801\n",
      "epoch 3 batch id 66881 loss 1.06415855884552 train acc 0.5236880616318536\n",
      "epoch 3 batch id 67261 loss 0.9573778510093689 train acc 0.5237324099403815\n",
      "epoch 3 batch id 67641 loss 1.271979570388794 train acc 0.5237658650079094\n",
      "epoch 3 batch id 68021 loss 1.1367019414901733 train acc 0.523781258728922\n",
      "epoch 3 batch id 68401 loss 1.0767351388931274 train acc 0.5237946539524276\n",
      "epoch 3 batch id 68781 loss 0.8838461637496948 train acc 0.5238142619328012\n",
      "epoch 3 batch id 69161 loss 1.027764081954956 train acc 0.523823713870534\n",
      "epoch 3 batch id 69541 loss 0.9448715448379517 train acc 0.5238269959448383\n",
      "epoch 3 batch id 69921 loss 1.0155999660491943 train acc 0.5238394044707598\n",
      "epoch 3 batch id 70301 loss 0.9019715189933777 train acc 0.5238503453009203\n",
      "epoch 3 batch id 70681 loss 0.9028448462486267 train acc 0.5238830537909763\n",
      "epoch 3 batch id 71061 loss 1.0635731220245361 train acc 0.5238643999521538\n",
      "epoch 3 batch id 71441 loss 1.1054108142852783 train acc 0.523863441511177\n",
      "epoch 3 batch id 71821 loss 0.9934970140457153 train acc 0.5238968668634522\n",
      "epoch 3 batch id 72201 loss 0.8769723773002625 train acc 0.5239147916926359\n",
      "epoch 3 batch id 72581 loss 0.9251006841659546 train acc 0.5239508273515108\n",
      "epoch 3 batch id 72961 loss 0.9892082214355469 train acc 0.5239757798686969\n",
      "epoch 3 batch id 73341 loss 1.0042389631271362 train acc 0.5239857736463915\n",
      "epoch 3 batch id 73721 loss 1.1248445510864258 train acc 0.523986550643643\n",
      "epoch 3 batch id 74101 loss 1.1109939813613892 train acc 0.5240204248255759\n",
      "epoch 3 batch id 74481 loss 0.9089581370353699 train acc 0.5240615056188827\n",
      "epoch 3 batch id 74861 loss 1.0226397514343262 train acc 0.5240850543006372\n",
      "epoch 3 batch id 75241 loss 0.9518318176269531 train acc 0.5240919595034622\n",
      "epoch 3 batch id 75621 loss 0.9071654677391052 train acc 0.5241299953055368\n",
      "epoch 3 batch id 76001 loss 1.1024757623672485 train acc 0.5241289999473692\n",
      "epoch 3 batch id 76381 loss 1.082977056503296 train acc 0.5241501076838481\n",
      "epoch 3 batch id 76761 loss 1.1230692863464355 train acc 0.5241893262854835\n",
      "epoch 3 batch id 77141 loss 1.0693236589431763 train acc 0.5242081059358836\n",
      "epoch 3 batch id 77521 loss 0.9354478716850281 train acc 0.5242250890081397\n",
      "epoch 3 batch id 77901 loss 1.0713094472885132 train acc 0.5242541414744355\n",
      "epoch 3 batch id 78281 loss 0.9534779191017151 train acc 0.5242709357954037\n",
      "epoch 3 batch id 78661 loss 1.154469609260559 train acc 0.5242980956255323\n",
      "epoch 3 batch id 79041 loss 1.1160494089126587 train acc 0.5242981095254362\n",
      "epoch 3 batch id 79421 loss 1.0926767587661743 train acc 0.52428867994611\n",
      "epoch 3 batch id 79801 loss 0.9892120957374573 train acc 0.5243063605092668\n",
      "epoch 3 batch id 80181 loss 0.9815022349357605 train acc 0.5243075042715856\n",
      "epoch 3 batch id 80561 loss 0.9835276007652283 train acc 0.5242960303372599\n",
      "epoch 3 batch id 80941 loss 0.946495771408081 train acc 0.5243066709084395\n",
      "epoch 3 batch id 81321 loss 0.9987669587135315 train acc 0.5243054915089583\n",
      "epoch 3 batch id 81701 loss 1.0210821628570557 train acc 0.5243337749843943\n",
      "epoch 3 batch id 82081 loss 1.080531358718872 train acc 0.5243389532900428\n",
      "epoch 3 batch id 82461 loss 0.9506319761276245 train acc 0.5243520421775142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 82841 loss 0.9359716773033142 train acc 0.5243538827392233\n",
      "epoch 3 batch id 83221 loss 1.015337586402893 train acc 0.5243682859494598\n",
      "epoch 3 batch id 83601 loss 1.0107145309448242 train acc 0.5243969494982117\n",
      "epoch 3 batch id 83981 loss 0.9468413591384888 train acc 0.5244100972243722\n",
      "epoch 3 batch id 84361 loss 0.946188747882843 train acc 0.5244210891288629\n",
      "epoch 3 batch id 84741 loss 1.0942317247390747 train acc 0.524413912686893\n",
      "epoch 3 batch id 85121 loss 1.0090632438659668 train acc 0.5244236880440785\n",
      "epoch 3 batch id 85501 loss 1.0491913557052612 train acc 0.5244158328557561\n",
      "epoch 3 batch id 85881 loss 1.066368818283081 train acc 0.5244493470616318\n",
      "epoch 3 batch id 86261 loss 0.9466456174850464 train acc 0.5244488746362783\n",
      "epoch 3 batch id 86641 loss 1.0449713468551636 train acc 0.524472211481862\n",
      "epoch 3 batch id 87021 loss 0.8774000406265259 train acc 0.5244827757093116\n",
      "epoch 3 batch id 87401 loss 0.9691448211669922 train acc 0.5245027230809716\n",
      "epoch 3 batch id 87781 loss 0.8731328845024109 train acc 0.5245319317392146\n",
      "epoch 3 batch id 88161 loss 0.9212387204170227 train acc 0.524545823833668\n",
      "epoch 3 batch id 88541 loss 1.0698845386505127 train acc 0.5245375376944015\n",
      "epoch 3 batch id 88921 loss 0.9886043667793274 train acc 0.5245407440312188\n",
      "epoch 3 batch id 89301 loss 0.9652173519134521 train acc 0.5245274758961266\n",
      "epoch 3 batch id 89681 loss 1.0670212507247925 train acc 0.5245430679296618\n",
      "epoch 3 batch id 90061 loss 0.9281995892524719 train acc 0.524591492155317\n",
      "epoch 3 batch id 90441 loss 0.9095064997673035 train acc 0.524602710606915\n",
      "epoch 3 batch id 90821 loss 1.0984784364700317 train acc 0.5246246738089209\n",
      "epoch 3 batch id 91201 loss 1.2533591985702515 train acc 0.5246471392857535\n",
      "epoch 3 batch id 91581 loss 1.1920443773269653 train acc 0.524651503859971\n",
      "epoch 3 batch id 91961 loss 1.0279295444488525 train acc 0.5246476767325279\n",
      "epoch 3 batch id 92341 loss 1.1437020301818848 train acc 0.5246462500406104\n",
      "epoch 3 batch id 92721 loss 1.0696228742599487 train acc 0.5246534293741439\n",
      "epoch 3 batch id 93101 loss 0.9293031096458435 train acc 0.5246320192586545\n",
      "epoch 3 batch id 93481 loss 0.9311265349388123 train acc 0.5246320107829399\n",
      "epoch 3 batch id 93861 loss 1.0548027753829956 train acc 0.5246516457847242\n",
      "epoch 3 batch id 94241 loss 1.124139666557312 train acc 0.5246576927770291\n",
      "epoch 3 batch id 94621 loss 1.011663794517517 train acc 0.5246635260671522\n",
      "epoch 3 batch id 95001 loss 1.011094093322754 train acc 0.5246811546720561\n",
      "epoch 3 batch id 95381 loss 1.055342197418213 train acc 0.5247004447950849\n",
      "epoch 3 batch id 95761 loss 1.0727766752243042 train acc 0.5247073443259782\n",
      "epoch 3 batch id 96141 loss 0.886150598526001 train acc 0.5247140267939797\n",
      "epoch 3 batch id 96521 loss 0.9828640222549438 train acc 0.5247451008070783\n",
      "epoch 3 batch id 96901 loss 1.0321162939071655 train acc 0.5247420692252918\n",
      "epoch 3 batch id 97281 loss 1.0887038707733154 train acc 0.524748858975545\n",
      "epoch 3 batch id 97661 loss 1.0200861692428589 train acc 0.5247671153275105\n",
      "epoch 3 batch id 98041 loss 0.9762184023857117 train acc 0.5248040360665436\n",
      "epoch 3 batch id 98421 loss 1.1056694984436035 train acc 0.5248063802440536\n",
      "epoch 3 batch id 98801 loss 0.9438794851303101 train acc 0.5248147159441706\n",
      "epoch 3 batch id 99181 loss 1.0010297298431396 train acc 0.524831967564352\n",
      "epoch 3 batch id 99561 loss 1.160847544670105 train acc 0.5248373170719458\n",
      "epoch 3 batch id 99941 loss 1.0298223495483398 train acc 0.5248616996527952\n",
      "epoch 3 batch id 100321 loss 1.0308799743652344 train acc 0.5248725030153208\n",
      "epoch 3 batch id 100701 loss 0.9132606983184814 train acc 0.5248888106870836\n",
      "epoch 3 batch id 101081 loss 1.0665265321731567 train acc 0.5249003583759559\n",
      "epoch 3 batch id 101461 loss 1.06571364402771 train acc 0.5249158235676763\n",
      "epoch 3 batch id 101841 loss 1.0374454259872437 train acc 0.5249212006952013\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        # token_ids = token_ids.long()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        # segment_ids = segment_ids.long()\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        # label = label.long()\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        # token_ids = token_ids.long()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        # segment_ids = segment_ids.long()\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        # label = label.long()\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './kobert_jaso_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cafd226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "def predict(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"PER\"), 'bio_tag'] = 0\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"PER_B\"), 'bio_tag'] = 0\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"PER_I\"), 'bio_tag'] = 0\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"ADDR\"), 'bio_tag'] = 1\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"ADDR_B\"), 'bio_tag'] = 1\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"ADDR_I\"), 'bio_tag'] = 1\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"MON\"), 'bio_tag'] = 2\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"MON_B\"), 'bio_tag'] = 2\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"MON_I\"), 'bio_tag'] = 2\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"BIR\"), 'bio_tag'] = 3\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"BIR_B\"), 'bio_tag'] = 3\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"BIR_I\"), 'bio_tag'] = 3\n",
    "            tag_data.loc[(tag_data['bio_tag'] == \"O\"), 'bio_tag'] = 4\n",
    "\n",
    "            if np.argmax(logits) == 0:\n",
    "                test_eval.append(\"PER\")\n",
    "            elif np.argmax(logits) == 1:\n",
    "                test_eval.append(\"ADDR\")\n",
    "            elif np.argmax(logits) == 2:\n",
    "                test_eval.append(\"MON\")\n",
    "            elif np.argmax(logits) == 3:\n",
    "                test_eval.append(\"BIR\")\n",
    "            elif np.argmax(logits) == 4:\n",
    "                test_eval.append(\"O\")\n",
    "\n",
    "        print(\">> 태그 : \" + test_eval[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "end=1\n",
    "while end ==1 :\n",
    "  sentence = input(\"단어 입력 : \")\n",
    "  if sentence == 0:\n",
    "    break\n",
    "  predict(sentence)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a391409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e786a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
